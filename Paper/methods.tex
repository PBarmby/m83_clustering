%\section{Clustering Analysis}
As the size of galactic surveys grows, the number of dimensions available for analysis increases.
In this survey, 45 different colour combinations are possible, creating a space of 45 possible dimensions. 
Clustering methods provide an efficient way of finding structure in high dimentional data by searching for structure in the feature spaces that cannot be visually inspected. 
A feature space is a set of \textit{n} features that are associated with measurable quantities.
The following techniques were used to cluster the data, and determine the most significant features. 
All analysis was implemented using the \textit{sklearn.cluster} Python package. % Reference this properly 

\subsection{Princpal Component Analysis}
Images of objects at various wavelengths share similar structures. These structures can be built using a subset of only the most significant of those images.  
Princpal Component Analysis (PCA) is a process of determining the most significant features of a high dimentional space in order to rebuild structures effectively \citet{kuntzer16}.
PCA projects a data set on its most significant basis, only keeping the dimensions that best explain the data. 
These dimensions are kept by imposing a maximum number of components on the algorithm, and it is forced to only select the most signifcant ones \citet{kuntzer16}
Since the number of colour dimensions in this problem is not large, PCA was only considered when determining the most significant colours. 
The Explained Varience Ratio was used to determine each colour significance in each combination. % explain EVR here
PCA has been applied to many astronomical surveys... % list papers here and analysis
In this study, PCA is conducted to determine the most significant filters, and colours for classifying objects in M83.
All ten filters were used in the inital analysis... % Do PCA and write the conclusions here ??

\subsection{Mean Shift Clustering} 
Mean Shift is a non-parametric clustering technique that is based on probability density function estimates of each point in the data. % Should we mentioned sklearn.cluster as the implementation? 
Mean Shift is a very powerful algorithm, but has not been widely used in astronomy. % Find a paper using it 
At each point, the algorithm estimates the density around that point using a small sample of objects surrounding the point.
The power of Mean Shift clustering is that the clusters are not confined to a particular shape.
Because Mean Shift moves towards the local mode near the data on which it was initialized, it is useful for estimating the number of signifcant clusters in a dataset \citet{comanciciu02}
The algorithm is based on two components: kernal density estimation, and density gradient estimation.
We will highlight the major components of the algorithm, for a full description of the, see \citet{vatturi09}.

The first element of Mean Shift is kernel density estimation. 
The major parameter of Mean Shift is bandwidth, \textbf{\textit{H}}, which is assumed to be proportional to the matrix $\textbf{\textit{H}} = h^2\textbf{\textit{I}}$, with $h>0$ \citet{vatturi09}.
The desnity estimator for a multivariate density kernal is given by: 
\begin{equation} 
\label{eq:ms_de}
\hat{f}(x) = \frac{c_{k,d}}{nh^d} \sum_{i=1}^n k\Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)
\end{equation}
Where $h$ is the magnitude of the bandwidth matrix, $k(x)$ is the profile of kernal $K(x)$, and $c_{k,d}$ is a constant making $K(x)$ integrate to one\citet{vatturi09}.

The second element of Mean Shift is density gradient estimation. 
The density gradient is estimated from the gradient of equation~\ref{eq:ms_de}\citet{vatturi09}.
The density gradient is given by: 
\begin{multline}
\label{eq:ms_dg}
\nabla\hat{f}_{h,K}(x) = \frac{2c_{k,d}}{nh^(d+2)} \Bigg[ \sum_{i=1}^n k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg) \Bigg] \\ \Bigg[\frac{\sum_{i=1}^n x_{i}k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)}{\sum_{i=1}^n k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)} - x \Bigg]
\end{multline}
The second term of equation~\ref{eq:ms_dg}, is the Mean Shift; the difference between the weighted mean using $k^{\prime}$, and $x$ \citet{vatturi09}.
Applying a normal kernal to the Mean Shift, the second term of equation~\ref{eq:ms_dg} becomes: 
\begin{equation} 
\label{eq:ms_sh}
m_{h,K}(x) = \frac{\sum_{i=1}^n x_{i} exp \Bigg(\norm{\frac{x-x_{i}}{h}}^2 \Bigg)}{\sum_{i=1}^n exp \Bigg(\norm{\frac{x-x_{i}}{h}}^2 \Bigg)} - x
\end{equation}
$m_{h,K}$ is the Mean Shift, and always points in the direction of largest ascent through the estimated density function \citet{vatturi09}.

Mean Shift clustering involves the application of equation~\ref{eq:ms_sh} to shift the points of a data set towards the direction of the Mean Shift vector \citet{vatturi09}. 
The points are shifted by: 
\begin{equation}
\label{eq:ms}
x^{i+1} = x^i + m_{h,K}(x^i)
\end{equation}
Shifting the data points by equation~\ref{eq:ms} ensures that when the points converge, the center is the area of highest local density, or density ``mode". 
The density mode can be interpretted as the center of a significant cluster in the data set, and is used to classify the objects that were shifted towards it.
Equation~\ref{eq:ms_de} introduces the bandwidth parameter $h$. 
Estimating the bandwidth correctly is critical to determining the correct number of clusters.
If the bandwidth is too low, the density estimate will be undersmoothed, and Mean Shift will produce many small clusters \citet{vatturi09}. This is a result of the large density gradient resulting from a low bandwidth, causing many data points to be interpreted as local modes.
% I'm not sure if my logic here is correct - take a look to make sure
Conversly, if the bandwidth is too large, a small number of large clusters will be detected, resulting in groupings of data that may blur the underlying structure \citet{vatturi09}.
% Add section about bandwidth estimation
% Discuss drawbacks

\subsection{Affinity Propagation Clustering}

Affinity propagation (AP) is a relatively new clustering technique developed by \citet{frey07}.
Here, we will breifly describe the main compoments of AP, for a full description of the technique, see \citet{frey07}.
AP takes the similarities between the data points as input for clustering, and uses a series of "messages" between data points to determine the number of clusters and their centers.
The centers of AP clustering are actual data points, called exemplars, which make it useful for clustering as it does not create average centers for each cluster. 
The only input required for AP  are the \textit{preferences} of each data point which describes the likelihood of a data point to be chosen as an exemplar \citet{frey07}.
The preferences are a measure of the similarity between a point \textit{i} and a candidate exemplar \textit{k} defined by: 
\begin{equation}
\label{eq:sim}
s(i,k) = -\norm{x_i - x_k}^2
\end{equation}
Similarity values influence the number of clusters AP identifies, as the larger similarity values are likely chosen as eemplars \citet{frey07}.
If all data points are equally suitable to be used as exemplars, the preference value could be common among all data points. This technique was adopted as no prior knowledge of the data set was used to determine where clusters were centered. 
Preference values could be estimated using the median value of the similarities, the minimum value, or randomized to see the effects over various clusterings \citet{frey07}.

Once the preference value is determined, two messages are computed between all the data points.
The first message is the "responsibility" $r(i,k)$, which is sent from point \textit{i} to candidate exemplar \textit{k}:\citet{frey07}

\begin{equation}
\label{eq:resp}
r(i,k) \leftarrow s(i,k) - max_{k^\prime s.t,  k^\prime \neq k} \{ a(i,k^\prime) + s(i,k^\prime) \}
\end{equation}

Responsibility measures the evidence of how suitable point \textit{k} is to be an exemplar of point \textit{i} \citet{frey07}, after considering other potential exemplars for point \textit{i}.
The "availability", $a(i,k^\prime)$ in equation~\ref{eq:resp}, is sent from candidate exemplar \textit{k} to point \textit{i} to compute the evidence for how appropriate it would be for point \textit{i} to choose candidate \textit{k} as an exemplar, considering evidence from other points that believe candidate \textit{k} should be their exemplar \citet{frey07}: 

\begin{equation}
\label{eq:avail}
a(i,k) \leftarrow min\Big\{ 0, r(k,k) + \sum\limits_{i^\prime s.t, i^\prime \notin \{i,k\}} max\{0, r(i^\prime, k)\}\Big\}
\end{equation}

The availabilities of all points are initialized to zero, and the first iteration of responsibilities are set to the input preferences \citet{frey07}. 
Each iteration updates equation~\ref{eq:resp} and equation~\ref{eq:avail} to determine the optimal exemplars for the data.
As the availabilities (Equation~\ref{eq:avail}) are updated, a threshold is calculated to ensure that the availability for a given candidate does not become positive: 

\begin{equation}
\label{eq:selfa}
a(k,k) \leftarrow \sum\limits_{i^\prime s.t., i^\prime \neq k} max\Big\{0,r(i^\prime, k)\Big\}
\end{equation}

Equation~\ref{eq:selfa} is called the "self-availability", and reflects the evidence that \textit{k} is an exemplar, based on the responsibilities sent to \textit{k} from other points \citet{frey07}.
The self-availability limits the influence of strong candidates, to ensure that false exemplars are not chosen.

As the process iterates, for point \textit{i}, the value of \textit{k} that maximizes $a(i,k) + r(i,k)$ identifies \textit{i} as an exemplar if $k=i$, or gives the exemplar of point \textit{i} \citet{frey07}.
In order to ensure that the message passing does not cause numerical oscilations, the messages are damped as they are updated. The previous message value is  multiplied by a damping-factor $\lambda$, and $1 - \lambda$ multiplied by the update value is added. The damping-factor has a value between zero and one, with a default value of 0.5 \citet{frey07}.

\subsection{K-Means Clustering}
K-Means clustering is one of the most widely used clustering methods and has been used to identify a wide range of interstellar and intergalactic objects. % List papers that have done so
It is simple, robust, and easy to implement when analyzing high dimentional spaces, making it a powerful way to analyze galactic surveys. 
Generally, k-means begins by selecting $k$ data points at random and deems these points cluster centers.
Each object in the data set is then assigned to a cluster center by computing the least-squares distance to each center.
K-Means aims to minimize the sum of squares within each cluster given by:
\begin{equation}
\label{eq:km_sos}
J = \sum_{n=1}^N \sum_{k=1}^K min \big( \norm{x_n - \mu_k}^2 \big)
\end{equation}
Each point, $x$, is then assigned to the cluster center with the lowest distance in equation~\ref{eq:km_sos}\citet{tammour16}.
Once all data points have been assigned, the centers are re-calculated by taking the average of all the points in each cluster. 
This process continues until the centers do not change after two consecutive iterations \citet{sanchez-almeida13}. 

% Discuss drawbacks
% K-Means requires the number of clusters to be inputed by the user. 
% This posses a challenge for high dimentional data, as the number of clusters cannot be estimated by visual inspection. 

\begin{enumerate}
\item description of clustering and classification 
\item description of PCA
\item description of Mean-Shift
\item description of Affinity Propagation
\item description of K-Means
\item experiments with how to apply the techniques
\item final parameters used
\end{enumerate}
