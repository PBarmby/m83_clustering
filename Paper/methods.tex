%\section{Clustering Analysis}
As the size of photometric surveys grows, the number of dimensions available for analysis increases.
Clustering methods provide an efficient way of finding structure in high dimensional data by searching for structure in colour spaces that are difficult to visualize. 
Three different techniques were used to cluster the data;
all were implemented using the \textit{sklearn.cluster} Python package version 0.17 \citep{sklearn}. %AKK: please check that this is the right version number
%AKK: is AP in sklearn - or did you implement this yourself?

\subsection{Mean Shift Clustering}
\label{sec:methods_ms}

Mean Shift is a non-parametric clustering technique that is based on probability density function estimates at each point in the data. 
Mean Shift is a very powerful algorithm, but has not been widely used in astronomy. % Find a paper using it
The power of Mean Shift clustering is that the clusters are not confined to a particular shape.
Because Mean Shift moves towards the local mode near the data on which it was initialized, it is useful for estimating the number of significant clusters in a dataset \citep{comanciciu02}.
At each point, the algorithm estimates the density around that point using a small sample of objects surrounding the point.
The algorithm is based on two components: kernel density estimation and density gradient estimation.
The following highlights the major components of the algorithm; for a full description, see \citet{vatturi09}.

The first element of Mean Shift is kernel density estimation. 
The major parameter of Mean Shift is bandwidth, \textbf{\textit{H}}, which is assumed to be proportional to the matrix $\textbf{\textit{H}} = h^2\textbf{\textit{I}}$, with $h>0$ \citet{vatturi09}.
The density estimator for a multivariate density kernel is given by: 
\begin{equation} 
\label{eq:ms_de}
\hat{f}(x) = \frac{c_{k,d}}{nh^d} \sum_{i=1}^n k\Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)
\end{equation}
where $x_i$ are a set of $n$ independent \textit{d}-dimensional data points, $h$ is the magnitude of the bandwidth matrix, $k(x)$ is the profile of kernel $K(x)$, and $c_{k,d}$ is a normalization constant. %AKK: unclear what difference between $K$ and $k$ is
Estimating the bandwidth correctly is critical to determining the correct number of clusters.
If the bandwidth is too low, the density estimate will be undersmoothed, and Mean Shift will produce many small clusters.
Conversely, if the bandwidth is too large, a small number of large clusters will be detected, resulting in groupings of data that may blur the underlying structure \citet{vatturi09}.

The second element of Mean Shift is density gradient estimation. 
The density gradient is estimated from the gradient of equation~\ref{eq:ms_de} and given by: 
\begin{multline}
\label{eq:ms_dg}
\nabla\hat{f}_{h,K}(x) = \frac{2c_{k,d}}{nh^(d+2)} \Bigg[ \sum_{i=1}^n k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg) \Bigg] \\ \Bigg[\frac{\sum_{i=1}^n x_{i}k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)}{\sum_{i=1}^n k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)} - x \Bigg]
\end{multline}
The second term of equation~\ref{eq:ms_dg}, is the Mean Shift, the difference between the weighted mean using $k^{\prime}$ and $x$.
Applying a normal kernel to the Mean Shift, the second term of equation~\ref{eq:ms_dg} becomes: 
\begin{equation} 
\label{eq:ms_sh}
m_{h,K}(x) = \frac{\sum_{i=1}^n x_{i} \exp \Bigg(\norm{\frac{x-x_{i}}{h}}^2 \Bigg)}{\sum_{i=1}^n \exp \Bigg(\norm{\frac{x-x_{i}}{h}}^2 \Bigg)} - x
\end{equation}
$m_{h,K}$ is the Mean Shift, and always points in the direction of largest ascent through the estimated density function \citet{vatturi09}.

Mean Shift clustering involves the iterative application of equation~\ref{eq:ms_sh} to shift the points of a data set towards the direction of the Mean Shift vector.
In each iteration $j$ the points are shifted by: 
\begin{equation}
\label{eq:ms}
x_i^{j+1} = x_i^j + m_{h,K}(x_i^j)
\end{equation}
Shifting the data points by equation~\ref{eq:ms} ensures that when the points converge, the center is the area of highest local density, or density ``mode". 
The density mode can be interpreted as the centre of a significant cluster in the data set and is used to classify the objects that were shifted towards it.
% Add section about bandwidth estimation
% Discuss drawbacks

\subsection{Affinity Propagation Clustering}
\label{sec:methods_ap}

Affinity propagation (AP) is a relatively new clustering technique developed by \citet{frey07}.
The main components are described here; for a full description of the technique, see \citet{frey07}.
AP takes the similarities between the data points as input for clustering, and uses a series of ``messages'' between data points to determine the number of clusters and their centers.
The centres of AP clustering are actual data points, called exemplars; unlike other methods AP does not create average centres for each cluster. 
The first inputs required for AP are the \textit{preferences} of each data point which describe the likelihood of a data point to be chosen as an exemplar.
The preferences are a measure of the similarity between a point \textit{i} and a candidate exemplar \textit{k} defined by: 
\begin{equation}
\label{eq:sim}
s(i,k) = -\norm{x_i - x_k}^2
\end{equation}
Similarity values influence the number of clusters AP identifies, as the larger similarity values are likely chosen as exemplars.
Preference values could be estimated using the median value of the similarities, the minimum value, or randomized to see the effects over various clusterings.
%how does our implementation estimate preference values?

Once the preference value is determined, two messages are computed between all the data points.
The first message is the ``responsibility'' $r(i,k)$, which is sent from point \textit{i} to candidate exemplar \textit{k}:
\begin{equation}
\label{eq:resp}
r(i,k) \leftarrow s(i,k) - max_{k^\prime s.t,  k^\prime \neq k} \{ a(i,k^\prime) + s(i,k^\prime) \}
\end{equation}
Responsibility measures the suitability of point \textit{k} to be an exemplar of point \textit{i} \citet{frey07}, after considering other potential exemplars for point \textit{i}.
The ``availability'', $a(i,k^\prime)$ in Equation~\ref{eq:resp}, is sent from candidate exemplar \textit{k} to point \textit{i} to compute the evidence for how appropriate it would be for point \textit{i} to choose candidate \textit{k} as an exemplar, considering evidence from other points that believe candidate \textit{k} should be their exemplar: %rewrite to avoid implying that data points ``believe'' anything
\begin{equation}
\label{eq:avail}
a(i,k) \leftarrow min\Big\{ 0, r(k,k) + \sum\limits_{i^\prime s.t, i^\prime \notin \{i,k\}} max\{0, r(i^\prime, k)\}\Big\}
\end{equation}

The availabilities of all points are initialized to zero, and the first iteration of responsibilities are set to the input preferences.
Each iteration updates equation~\ref{eq:resp} and equation~\ref{eq:avail} to determine the optimal exemplars for the data.
As the process iterates, for point \textit{i}, the value of \textit{k} that maximizes $a(i,k) + r(i,k)$ identifies \textit{i} as an exemplar if $k=i$, or gives the exemplar of point \textit{i} \citet{frey07}.
In order to ensure that the message passing does not cause numerical oscillations, the messages are damped as they are updated.
The previous message value is multiplied by a damping factor $0 <\lambda<1$, and $1 - \lambda$ multiplied by the update value is added. %AKK: pls clarify second part of this sentence
The damping factor has a default value of 0.5. %AKK: what value did we use? Does it change as the algorithm runs?


\subsection{K-Means Clustering}
K-Means clustering is one of the most widely used clustering methods and has been used to identify a wide range of interstellar and intergalactic objects. % List papers that have done so
It is simple, robust, and easy to implement when analyzing high dimensional spaces, making it a powerful way to analyze multi-band photometric surveys. 
With the k-means algorithm, the number of clusters $K$ must be selected in advance.
K-Means aims to minimize the sum of squares of distances within each cluster given by:
\begin{equation}
\label{eq:km_sos}
J = \sum_{i=1}^N \sum_{k=1}^K \min \big( \norm{x_i - \mu_k}^2 \big)
\end{equation}
The algorithm is initialized by selecting $K$ data points at random and deems these points cluster centres, denoted by $\mu_k$. 
Each point in the data set $x_i$ is then assigned to a cluster centre by finding the centre to which the least-squares distance is the smallest.
The algorithm iterations continue with the centres being re-calculated by taking the average position of all the points in each cluster and reassigning each point to the nearest cluster centre.
The stopping criterion is that the centres do not change after two consecutive iterations \citep{sanchez-almeida13}. %change by how much? why are we citing this particular paper?

% Discuss drawbacks
% K-Means requires the number of clusters to be inputed by the user. 
% This posses a challenge for high dimentional data, as the number of clusters cannot be estimated by visual inspection. 
