% Analysis section
This section will outline colour construction, the clustering process, and the selection of the optimal clustering for each colour combination.
Each colour combination was clustered using the same process, and in two and three dimensions.


\subsection{Colour Selection}

Typical observations of nearby galaxies involve three or four filters, % would be nice to have some backup for this statement
which can be used to construct two and three independent sets of colours, respectively. %CHECK
The large number of different filters used for the ERS observations allow us to explore which combinations of four filters would be most useful.
With four filters $ABCD$, colours can be constructed in either two dimensions ($A-B$ versus $C-D$) or three ($A-B$ versus $B-C$ versus $C-D$ and other combinations).
Both variations were considered in the clustering analysis.
Observations in 10 bands allow the generation of 45 different colours, but not all of these colours are likely to be useful in characterizing components of the galaxy.

Two types of band combinations were created: combinations of the most commonly-used broad band filters and combinations including one narrow band and three broad band filters.
A common band was used in the three-dimensional colours to create a three dimensional space with colours that could be transformed back into the original two dimensional space. 
In the broad band combinations, three dimensional colour spaces were created by making colours with a common band, either $B$ or $V$.
In the narrow band combinations, the narrow band was used as the common band.
Clustering in three dimensions increased the complexity of the distribution, creating more information for the clustering algorithms to use. %need to reword this: not really ``creating more information''..
Although the original ERS catalog contained nearly 70000 objects, not all objects were detected in all bands with sufficient signal-to-noise for reliable colours. 
For a given combination of bands, we used only objects for which the magnitude uncertainty was $<0.2$~mag in each band.
Tables~\ref{tab:BBcolours}~and~\ref{tab:NBcolourcombos} list the two dimensional colour combinations, the number of objects detected in each colour and their mean uncertainty, and the number of objects detected in the colour combination.


\subsubsection{Broad Band Combinations}

The first type of combination was comprised of the broad band filters: F336W, F438W, F555W, and F814W.
For readability, we refer to these hereafter as $U$, $B$, $V$ and $I$, although these filters do not correspond exactly to the ground-based equivalents.
These bands are used in many {\textit HST} studies and had the largest number of detections in the ERS catalog; broadband visible-light colours
are rough indicators of stellar temperatures in the range XX-YY~K. % COMPLETE
Although also a broad-band filter, the UV-wide filter F225W (hereafter UVW) is less-commonly used in the literature, and for the purposes of creating colour combinations we treated it as a narrow-band filter.
When creating colours from the broad bands, we did not use $U - I$, also not commonly-used in the literature.
The broad band colour combinations created using $UBVI$ are listed  in Table~\ref{tab:BBcombinations}.
These combinations were created in order to remove any obvious correlation between the colours that could occur due to the inclusion of the same band in both colours.
% not quite happy with prev sentence

\begin{table*}
\centering
\caption{Broad band colour combinations: detections in individual colours and combinations}
\label{tab:BBcombinations}
\begin{tabular}{lllllll}
\hline\hline
Colour 1 & $N_1^1$ & Mean Uncertainty & Colour 2 & $N_2$ & Mean Uncertainty & $N_{12}$ \\
 & & mag & & mag & \\
\hline
$U - B$ &  33523 & 0.1606  & $V - I$ &  57935 & 0.1334  & 28931\\
$U - V$ &  33692 & 0.1429  & $B - I$ &  41413 & 0.1590  & 28931\\
$B - V$ &  48660 & 0.1456  & \nodata & \nodate & \nodata & \nodata \\ % remind me again why this is largely blank?
\hline
\end{tabular}
$^1$: $N$ is number of objects detected with photometric uncertainties $<0.2$~mag.
\end{table*}

\subsubsection{Narrow Band Combinations}

\textbf{PB: Should we explain more about why we choose to do Broad - Narrow? And what objects we hope to find in these combos?}

The second set of combinations included the narrow band filters: F373N ($O_{2}$), F487N ($H\beta$), F502N ($O_{3}$), F657N ($H\alpha$), F673N ($S_{2}$), and the broad band F225W (UVW).
\textbf{Explain why narrow - broad}
Colours were created with the narrow bands by pairing them with the broad band which covered their peak in wavelength space.
This was done to separate objects that were emission line dominated, from objects that were continuum dominated.
Making colours out of a combination of broad and narrow bands ensure that the objects clustered in these bands are physically meaningful, as it is likely that the object would emit in the broad band that contains the narrow band. \textbf{Not sure if that is a reason why we chose to construct them that way}.
The second colour in each combination was created from two broad bands that did not overlap the first colour in wavelength space.
Table~\ref{tab:NBcolourcombos} lists the narrow band colour combinations used for analysis.
The number of objects in the narrow band combination, with the exception of the $H\alpha$ band, is significantly lower than the broad band combinations.
These combinations were useful for analysis as their distributions were not as dense as the broad bands, and the clustering algorithms were able to detect interesting structure within them.

\begin{table*}
\centering
\caption{Narrow band colour combinations and the number of objects detected in each colour, and in each combination, with uncertainties less than $0.2$.}
\label{tab:NBcolourcombos}
\begin{tabular}{lllllll}
\hline\hline
$Narrow - Broad$ & Objects & Mean Uncertainty & $Broad - Broad$ & Objects & Mean Uncertainty & Combined Objects \\
\hline
$UVW - U$ &  14977 & 0.1539 mag & $B - V$ &  48660 & 0.1456 mag & 14943 \\
$ - $ & $ - $ & $ - $ & $B - I$ &  41413 & 0.1590 mag & 14095 \\
$ - $ & $ - $ & $ - $ & $V - I$ &  57935 & 0.1334 mag & 14098 \\
\hline
$U - O_{2}$ & 8675 & 0.1504 mag & $B - V$ &  48660 & 0.1456 mag & 8657 \\
$ - $ & $ - $ & $ - $ & $B - I$ &  41413 & 0.1590 mag & 8558 \\
$ - $ & $ - $ & $ - $ & $V - I$ &  57935 & 0.1334 mag & 8559 \\
\hline
$B - H\beta$ & 13269 & 0.1493 mag & $V - I$ &  57935 & 0.1334 mag & 13147 \\
\hline
$O_{3} - V$ & 14644 & 0.1418 mag & $U - B$ &  33523 & 0.1606 mag & 13390 \\
\hline
$H\alpha - I$ & 59465 & 0.1495 mag & $U - B$ &  33523 & 0.1606 mag & 28920 \\
$ - $ & $ - $ & $ - $ & $U - V$ &  33692 & 0.1429 mag & 29060 \\
$ - $ & $ - $ & $ - $ & $B - V$ &  48660 & 0.1456 mag & 41317 \\
\hline
$S_{2} - I$ & 25185 & 0.1535 mag & $U - B$ &  33523 & 0.1606 mag & 14577 \\
$ - $ & $ - $ & $ - $ & $U - V$ &  33692 & 0.1429 mag & 14586 \\
$ - $ & $ - $ & $ - $ & $B - V$ &  48660 & 0.1456 mag & 18882 \\
\hline
\end{tabular}
\end{table*}

\subsection{Clustering Process}
Clustering was performed using all methods for each colour combination. 
The following process allowed the investigation of the effect of all parameters on each clustering technique.
This process identified the clustering that was most successful at identifying different segments of objects in the colour space.

\subsubsection{Mean shift}
Mean-Shift clustering was performed first by estimating the bandwidth parameter with the $estimate-bandwidth$ function in $scikit-learn$ \citep{sklearn}.
This function estimates the bandwidth parameter based on the distances between points in the dataset, and determines if the distribution has high or low variance.
Following the initial clustering, the bandwidth was varied and the clustering performed again to determine how sensitive a combination was to the parameter. 
The bandwidth values were changed on intervals of $\pm 0.1$ or $\pm 0.05$ from the estimated bandwidth value depending on a combination's sensitivity to the parameter. 
If a combination was very sensitive to bandwidth, then the number of clusters that mean shift would predict would vary greatly over a small range of bandwidth values.
This type of combination usually resulted in poor segmentation, as the algorithm would not converge on a number of clusters. 
However, sensitivity could also be the result of the starting bandwidth estimate.
If the original estimate was in an unstable bandwidth interval, then the hierarchy would reflect that, and the testing of multiple bandwidth values could result in convergence.

\subsubsection{Affinity Propagation}
Affinity Propagation clustering was performed after Mean shift.
Affinity Propagation requires two parameters, the preferences, and the damping factor, outlined in Section~\ref{sec:methods_ap}.
The initial clusterings were performed under two independent conditions.
First, the preferences were set to the median value of the similarity between data points, and the damping factor was kept at the \textit{scikit-learn} default value of $0.5$.
Second, the preferences were set to the minimum value of the similarities, and the damping factor kept at the default value of $0.5$.
These clusterings resulted in a segmentation with over 100 clusters in multiple colour combinations, which was clearly not meaningful.

Following the two initial conditions, the preferences were set to 10\% of the number of objects in the data set, and the damping factor was set at $0.95$.
With these parameters, the clusterings varied significantly over different colours.
The clusterings were repeated by varying the damping factor and the preferences to try and reveal a trend in the parameters, but the algorithm was too sensitive for this size of dataset.
Following the initial tests of Affinity Propagation, it was determined that this clustering method was not effective.
Due to the number of computations required for the calculation of the messages passed between points on each iteration, the algorithm was very sensitive to the input parameters, and did not produce meaningful clusterings.
The algorithm is effective for small and medium sized datasets, and was able to create some reasonable clusters when the uncertainty limit was set at $0.1 mag$, which reduced the number of objects significantly. 
After multiple clusterings, a systematic way of determine the correct number of clusters could not be determined, and the algorithm was not used further.

\subsubsection{K-Means}
K-Means clustering was performed last.
The first clustering was performed using the number of clusters determined from the initial clusterings by Mean shift.
Next, K-Means was performed with $K = \pm 4$ from the original clustering.
This method of clustering was similar to the Mean shift approach, as it revealed how the dataset reacted to different values of $K$.
K-Means was the most efficient algorithm of the three, as it produced clusterings quickly, and always produced clusters of reasonable size.

Each K-Means clustering was checked by plotting the sum-of-squares value for each value of $K$.
As $K$ increases, the inertia value decreases as the inertia value represent the total distance between the total distance between the points of each cluster.
The value decreases with $K$ as the total distance in each cluster decreases as more clusters are introduced.
This distribution was used to check the clustering and ensure that the clusterings were successful.

Following the inertia test, the cluster centers were tested by running K-Means for 40 trials with the same value of $K$.
This test was run to determine if the clusterings were stable as K-Means is initialized randomly, and the clusters produced can depend on the starting position.
It was clear that each initialization found different clusters first depending on the starting point. 
Despite the random initialization the final cluster centers did not change.
A strong clustering was found when the variance in cluster centers was not large \textbf{quantify}.
If the cluster centers vary across multiple initializations of K-Means, the clustering would not be reliable, and the combination would not be considered for analysis.

\subsection{Characterizing the Clusterings}
Determining the strongest clustering was the most difficult task of the analysis.
Selecting the optimal clustering can often seem arbitrary, as no ``right" answer is obvious.
In order to characterize each clustering, a variety of metrics and statistics were calculated to evaluate each method.
The relationships between a variety of clustering parameters were investigated to try and determine how they indicated the strongest clustering.
Since the performance of the algorithms was directly related to the parameters used as input, those relationships were critical for characterizing the clustering.
The objects in each cluster were then found in the white-light image of M83, to determine if there was a relationship between the objects assigned to the same cluster and their spatial position.
\textbf{Not sure if we want to mention the colour models here}
Finally, colour models were created and imposed on the cluster distribution to determine if the segmentation agreed with a model. \textbf{Need more on why the models were used}.

\subsubsection{Silhouette Score}
The silhouette score is a metric used to describe the compactness of a cluster in a given clustering and is calculated as an average of all samples in a clustering.  
The silhouette score is given by:

\begin{equation}
\label{eq:ss}
Silhouette Score = \frac{b - a}{\textit{max}\big(a, b\big)}
\end{equation}

where $a$ is the mean intra-cluster distance, and $b$ is the distance between a point and the nearest cluster that point is not a member of.
The score was used in two ways.
First, the average score was calculated for a given clustering.
This calculated the average score across all data in the sample.
Next, the average score for each cluster was computed.
The average cluster score evaluates the strength of a given cluster within a clustering.
This metric allowed each cluster to be evaluated individually, and revealed which clusters were responsible for the average score of the clustering.
Additionally, the average score allowed seemingly arbitrary clusters to be evaluated.
If the segmentation did not seem meaningful, the average score would reveal if the cluster was isolated and compact.
This revealed the significance of clusters that could have been viewed as noise or outliers. \textbf{This section needs more explanation}.

\textbf{Struggling to explain why that is how the score works}. 
Ideally, the silhouette score for the entire clustering should peak near the center of the distribution indicating the optimal clustering. 
High scores where the number of clusters is low often do not reflect the structure of the distribution, while high numbers of clusters are often imposed on the data as a result of the specified parameters.
This is often not the case, as seen in Figure~\ref{fig:sscore}, which shows the distribution of the silhouette score against the number of clusters.

For the K-Means clusterings, the score does not peak in the center of the distribution.
Instead of selecting the clustering with the highest score, the optimal clustering is found where the relation begins to elbow; between 4 and 5 clusters.
This clustering is selected because any increase in $K$ after this point does not affect the score, and does strengthen or weaken the clustering.
This means that the algorithm has found the balance between the natural clusters in the distribution and artificially segmenting the data.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figs/methods/UVW_U_score_vs_nclust}
\caption{Distribution of the silhouette score as a result of the number of clusters imposed for the $UVW - U$ and $V - I$ colours. The \textit{blue} points are the scores of Mean shift clustering, \textit{red} points are scores of K-Means.}
\label{fig:sscore}
\end{figure}

The distribution of score as a result of the Mean shift algorithm does not follow the same pattern.
The silhouette score was not as successful at describing the strength of Mean shift clusterings as Mean shift often created one large cluster and several smaller ones, which is not considered strong by the score.
In order to determine the optimal Mean shift clustering, other relationships were investigated.

The relations of the bandwidth parameter were investigated for each Mean shift clustering.
Figure~\ref{fig:bad_ms} shows the relation between the score and number of clusters for the three dimensional $U - O_{2}$ and $B - I$ combination.
The blue dots represent the Mean shift clusterings.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figs/methods/silhouette_score_bad_ms}
\caption{Distribution of the silhouette score as a result of the number of clusters imposed for the three dimensional $U - O_{2}$ and $B - I$ combination. The \textit{blue} points are the scores of Mean shift clustering, \textit{red} points are scores of K-Means.}
\label{fig:bad_ms}
\end{figure}

It is clear that an optimal clustering cannot be determined from this relation, as the score decreases linearly with the number of clusters Mean shift predicts.
The Mean-Shift scores do not follow a similar distribution as K-Means, as the accuracy of Mean shift is related to the bandwidth parameter, seen in Figure~\ref{fig:bwscore}.
The optimal Mean-Shift clustering was chosen by finding the bandwidth where the relation between the bandwidth and number of clusters reached an elbow, or where the relation between bandwidth and silhouette score elbowed.
In both panels of Figure~\ref{fig:bwscore} that the bandwidth parameter predicts five clusters.
Both distributions elbow at the same bandwidth interval, maximizing the score and predicting the same number of clusters for bandwidth values following the elbow.
The bandwidth parameter was the primary indicator of the optimal Mean shift clustering.
If a trend could not be found with the bandwidth parameter, then the silhouette score and number of clusters was investigated.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figs/methods/meanshift_parameters}
\caption{Distribution of the silhouette score as a function of bandwidth, and the distribution of the number of clusters as a function of bandwidth.}
\label{fig:bwscore}
\end{figure}

\subsubsection{Cluster Statistics}
Various statistics were calculated to help describe the similarity between the objects in a given cluster.
The standard deviation and average colour was calculated for each colour, and each cluster within a clustering. 
These metrics helped describe the distribution of the objects in the colour-colour space within a cluster. 
Clusters that had large standard deviations were viewed as too dissimilar to be a meaningful cluster, and clusters whose averages varied significantly from the cluster centers were discarded.

\textbf{I'm not sure that this paragraph describes why we calculated the fractional size}.
The fractional size of each cluster was also calculated and described the distribution of objects between clusters.
If a clustering segmented the objects into a large cluster followed by several smaller ones, the clustering was investigated further, as this segmentation could mean one of two things. 
This type of clustering could be a result of the identification of interesting objects, in which case the clustering algorithm was able to identify the objects and place them in the same cluster.
However, this type of clustering could also be a result of the underlying distribution of the data, as the clustering techniques are largely drawn to areas of high density.
If this is the case, the clustering only created the smaller clusters as a result of the parameters imposed on the clustering.
