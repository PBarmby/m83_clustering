%\section{Clustering Analysis}
As the size of galactic surveys grows, the number of dimensions available for analysis increases.
In this survey, 45 different colour combinations are possible, creating a space of 45 possible dimensions. 
Clustering methods provide an efficient way of finding structure in high dimentional data by searching for structure in the feature spaces that cannot be visually inspected. 
A feature space is a set of \textit{n} features that are associated with measurable quantities. 
In this study, each feature space is defined as a set of colours.

which can lead to the discovery of new patterns in spaces that we are already comfortable with. 
The methods used in this study are unspervised techniques. Unsupervised clustering takes the data independent of prior classification, and does not rely on prior labeling of the data.

\subsection{Princpal Component Analysis}
Images of objects at various wavelengths share similar structures. These structures can be built using a subset of only the most significant of those images.  
Princpal Component Analysis (PCA) is a process of determining the most significant features of a high dimentional space in order to rebuild structures effectively \citet{kuntzer16}.
PCA projects a data set on its most significant basis, only keeping the dimensions that best explain the data. 
These dimensions are kept by imposing a maximum number of components on the algorithm, and it is forced to only select the most signifcant ones \citet{kuntzer16}
PCA has been applied to many astronomical surveys... % list papers here and analysis
In this study, PCA is conducted to determine the most significant filters, and colours for classifying objects in M83.
All ten filters were used in the inital analysis... % Do PCA and write the conclusions here ??



\subsection{Mean Shift Clustering} 
Mean Shift is a non-parametric clustering technique that is based on probability density function estimates of each point in the data. % Should we mentioned sklearn.cluster as the implementation? 
Mean Shift is a very powerful algorithm, but has not been widely used in astronomy. % Find a paper using it 
At each point, the algorithm estimates the density around that point using a small sample of objects surrounding the point.
The power of Mean Shift clustering is that the clusters are not confined to a particular shape.
Because Mean Shift moves towards the local mode near the data on which it was initialized, it is useful for estimating the number of signifcant clusters in a dataset \citet{comanciciu02}
The algorithm is based on two components: kernal density estimation, and density gradient estimation.
We will highlight the major components of the algorithm, for a full description of the, see \citet{vatturi09}.

The first element of Mean Shift is kernel density estimation. 
The major parameter of Mean Shift is bandwidth, \textbf{\textit{H}}, which is assumed to be proportional to the matrix $\textbf{\textit{H}} = h^2\textbf{\textit{I}}$, with $h>0$ \citet{vatturi09}.
The desnity estimator for a multivariate density kernal is given by: 
\begin{equation} 
\label{eq:ms_de}
\hat{f}(x) = \frac{c_{k,d}}{nh^d} \sum_{i=1}^n k\Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)
\end{equation}
Where $h$ is the magnitude of the bandwidth matrix, $k(x)$ is the profile of kernal $K(x)$, and $c_{k,d}$ is a constant making $K(x)$ integrate to one\citet{vatturi09}.

The second element of Mean Shift is density gradient estimation. 
The density gradient is estimated from the gradient of equation~\ref{eq:ms_de}\citet{vatturi09}.
The density gradient is given by: 
\begin{multline}
\label{eq:ms_dg}
\nabla\hat{f}_{h,K}(x) = \frac{2c_{k,d}}{nh^(d+2)} \Bigg[ \sum_{i=1}^n k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg) \Bigg] \\ \Bigg[\frac{\sum_{i=1}^n x_{i}k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)}{\sum_{i=1}^n k^{\prime} \Bigg( \norm{\frac{x-x_{i}}{h}}^2 \Bigg)} - x \Bigg]
\end{multline}
The second term of equation~\ref{eq:ms_dg}, is the Mean Shift; the difference between the weighted mean using $k^{\prime}$, and $x$ \citet{vatturi09}.
Applying a normal kernal to the Mean Shift, the second term of equation~\ref{eq:ms_dg} becomes: 
\begin{equation} 
\label{eq:ms_sh}
m_{h,K}(x) = \frac{\sum_{i=1}^n x_{i} exp \Bigg(\norm{\frac{x-x_{i}}{h}}^2 \Bigg)}{\sum_{i=1}^n exp \Bigg(\norm{\frac{x-x_{i}}{h}}^2 \Bigg)} - x
\end{equation}
$m_{h,K}$ is the Mean Shift, and always points in the direction of largest ascent through the estimated density function \citet{vatturi09}.

Mean Shift clustering involves the application of equation~\ref{eq:ms_sh} to shift the points of a data set towards the direction of the Mean Shift vector \citet{vatturi09}. 
The points are shifted by: 
\begin{equation}
\label{eq:ms}
x^{i+1} = x^i + m_{h,K}(x^i)
\end{equation}
Shifting the data points by equation~\ref{eq:ms} ensures that when the points converge, the center is the area of highest local density, or density ``mode". 
The density mode can be interpretted as the center of a significant cluster in the data set, and is used to classify the objects that were shifted towards it.
Equation~\ref{eq:ms_de} introduces the bandwidth parameter $h$. 
Estimating the bandwidth correctly is critical to determining the correct number of clusters.
If the bandwidth is too low, the density estimate will be undersmoothed, and Mean Shift will produce many small clusters \citet{vatturi09}. This is a result of the large density gradient resulting from a low bandwidth, causing many data points to be interpreted as local modes.
% I'm not sure if my logic here is correct - take a look to make sure
Conversly, if the bandwidth is too large, a small number of large clusters will be detected, resulting in groupings of data that may blur the underlying structure \citet{vatturi09}.
% Add section about bandwidth estimation
% Discuss drawbacks

\subsection{Affinity Propagation Clustering}

Affinity propagation (AP) is a relatively new clustering technique developed by \citet{frey07}.
AP takes the similarities between the data points as input for clustering, and uses a series of "updates" to determine the number of clusters and their centers. 
The centers of AP clustering are actual data points, which makes it a useful method for segmentation as it does not create average centers for each cluster. 
Affinity propogation 

\subsection{K-Means Clustering}
K-Means clustering is one of the most widely used clustering methods and has been used to identify a wide range of interstellar and intergalactic objects. % List papers that have done so
It is simple, robust, and easy to implement when analyzing high dimentional spaces, making it a powerful way to analyze galactic surveys. 
Generally, k-means begins by selecting $k$ data points at random and deems these points cluster centers. 
Each object in the data set is then assigned to a cluster center by computing the least-squares distance to each center.
K-Means aims to minimize the sum of squares within each cluster given by:
\begin{equation} 
\label{eq:km_sos}
J = \sum_{n=1}^N \sum_{k=1}^K min \big( \norm{x_n - \mu_k}^2 \big)
\end{equation}
Each point, $x$, is then assigned to the cluster center with the lowest distance in equation~\ref{eq:km_sos}\citet{tammour16}.
Once all data points have been assigned, the centers are re-calculated by taking the average of all the points in each cluster. 
This process continues until the centers do not change after two consecutive iterations \citet{sanchez-almeida13}. 

% Discuss drawbacks
K-Means requires the number of clusters to be inputed by the user. 
This posses a challenge for high dimentional data, as the number of clusters cannot be estimated by visual inspection. 





\begin{enumerate}
\item description of clustering and classification 
\item description of PCA
\item description of Mean-Shift
\item description of Affinity Propagation
\item description of K-Means
\item experiments with how to apply the techniques
\item final parameters used
\end{enumerate}
